{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision.models import resnet50\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# custom imports\n",
    "from fairface_classification import data, resnet_model, utils, metrics, train\n",
    "from fairface_classification.data import Unnormalize\n",
    "\n",
    "# launch tensorboard writer\n",
    "# writer = SummaryWriter('../runs/fairface/')\n",
    "\n",
    "utils.set_random_seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = pd.read_csv('../data/fairface_label_train.csv')\n",
    "resnet = resnet50(weights='IMAGENET1K_V1')\n",
    "train_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_dict = data.read_json('../data/encodings/race_encoding.json')\n",
    "gender_dict = data.read_json('../data/encodings/gender_encoding.json')\n",
    "age_dict = data.read_json('../data/encodings/age_encoding.json')\n",
    "\n",
    "encoders = {'race' : race_dict, 'age' : age_dict, 'gender' : gender_dict}\n",
    "pprint(encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = json.load(open('../train_test_val.json'))['train']\n",
    "label_train_test = pd.read_csv('../data/fairface_label_train.csv')\n",
    "\n",
    "class_weights = utils.calc_tasks_weight({'age' : age_dict, \n",
    "                                         'gender' : gender_dict, \n",
    "                                         'race' : race_dict})\n",
    "train_labels = utils.calc_category_weights(train_samples, label_train_test, \n",
    "                                           {'age_dict' : age_dict, \n",
    "                                            'race_dict' : race_dict, \n",
    "                                            'gender_dict' : gender_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_class_weight(labels):\n",
    "    \"\"\"\n",
    "    It is another way of calculation of image weight for oversampling technique.\n",
    "\n",
    "    Args:\n",
    "        labels: pd.DataFrame - dataframe with filenames and its labels.\n",
    "    Returns:\n",
    "        labels: pd.DataFrame - dataframe with calculated weights for every file and its labels.\n",
    "    \"\"\"\n",
    "\n",
    "    labels['group'] = labels.apply(lambda x: f'{x[\"age\"]}{x[\"gender\"]}{x[\"race\"]}', axis=1)\n",
    "    group_vc = labels['group'].value_counts()\n",
    "    group_counts = group_vc.to_frame().reset_index()\n",
    "    labels = labels.merge(group_counts, on='group', how='left') \n",
    "    labels['group_weight'] = 1 / labels['count']\n",
    "    return labels\n",
    "\n",
    "train_labels = group_class_weight(train_labels)\n",
    "train_labels = train_labels.drop(['age_weight', 'gender_weight', 'race_weight', 'total_weight'], \n",
    "                                 axis=1)\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path('..')\n",
    "images_dir = 'data/'\n",
    "\n",
    "# Set the batch size for 64 samples\n",
    "batch_size = 2\n",
    "\n",
    "sampler = WeightedRandomSampler(weights=train_labels['group_weight'], num_samples=len(train_labels), replacement=True)\n",
    "\n",
    "# Create a FairFaceDataset object for the training data.\n",
    "fairface_train = data.FairFaceDataset('fairface_label_train.csv', \n",
    "                                        root_dir=root_dir, \n",
    "                                        mode='train',\n",
    "                                        encoders=encoders,\n",
    "                                        transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                      transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                                                      transforms.RandomPerspective(p=0.5),\n",
    "                                                                      transforms.RandomGrayscale(p=0.2),\n",
    "                                                                      transforms.RandomAdjustSharpness(2, p=0.5),\n",
    "                                                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                                                           std=[0.229, 0.224, 0.225])\n",
    "                                                                      ]))\n",
    "# Set the FairFaceDataset object to train mode.\n",
    "fairface_train.train()\n",
    "# Create a DataLoader object for the training data.\n",
    "train_loader = DataLoader(fairface_train, batch_size=batch_size, num_workers=5, sampler=sampler)\n",
    "\n",
    "# Create a FairFaceDataset object for the test data.\n",
    "fairface_test = data.FairFaceDataset('fairface_label_train.csv', \n",
    "                                     root_dir=root_dir, \n",
    "                                     mode='test',\n",
    "                                     encoders=encoders,\n",
    "                                     transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                   transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                                                        std=[0.229, 0.224, 0.225])]))\n",
    "# Set the FairFaceDataset object to train mode.\n",
    "fairface_test.train()\n",
    "# Create a DataLoader object for the test data.\n",
    "test_loader = DataLoader(fairface_test, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "\n",
    "# Create a FairFaceDataset object for the validation data.\n",
    "fairface_val = data.FairFaceDataset('fairface_label_val.csv', \n",
    "                                     root_dir=root_dir, \n",
    "                                     mode='val',\n",
    "                                     encoders=encoders,\n",
    "                                     transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                   transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                                                        std=[0.229, 0.224, 0.225])]))\n",
    "# Set the FairFaceDataset object to train mode.\n",
    "fairface_val.train()\n",
    "# Create a DataLoader object for the validation data.\n",
    "val_loader = DataLoader(fairface_val, batch_size=batch_size, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the backbone of ResNet50 on FairFace with Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from byol_pytorch import BYOL\n",
    "\n",
    "def create_models_dir():\n",
    "    models_dir = Path('../models')\n",
    "    if not models_dir.exists():\n",
    "        models_dir.mkdir()\n",
    "    return models_dir\n",
    "\n",
    "learner = BYOL(\n",
    "    resnet,\n",
    "    image_size = 224,\n",
    "    hidden_layer = 'avgpool'\n",
    ")\n",
    "\n",
    "opt = torch.optim.Adam(learner.parameters(), lr=3e-4)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for index, samples in enumerate(tqdm(train_loader)):\n",
    "        images = samples['image']\n",
    "        loss = learner(images)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        learner.update_moving_average() # update moving average of target encoder\n",
    "\n",
    "create_models_dir()\n",
    "# save your improved network\n",
    "torch.save(resnet.state_dict(), '../models/improved-net.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the fitted on backbone model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet = resnet.to(device)\n",
    "model = resnet_model.FairFaceResNet(resnet).to(device)\n",
    "\n",
    "# For race and age classes\n",
    "bce = nn.BCELoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[5, 10, 15, 20, 25], gamma=0.5, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
